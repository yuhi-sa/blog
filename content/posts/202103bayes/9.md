---
title: EMアルゴリズム
date: 2021-03-17T13:00:23+09:00
draft: false
tags: ["しくみがわかるベイズ統計と機械学習"] 
---
<!--more-->

あるフィットネスクラブにおける利用者の年齢分布のヒストグラムを考えると，筋トレを目的とした20代層とメタボ対策を目的とした50代層がおおきくなることが考えられる．

モデル化を考えた場合，このグラフの会員を2つのグループに分けそれぞれについて正規分布の最尤推定を行うことで2つの正規分布を得て，それらを重ね合わせて混合ガウスモデルを作成することを考える．

ここで，30代層は２つの山の裾野が重なる領域となるため扱いに困る．そこで，会員の目的は筋トレとメタボ対策の可能性が半分ずつとみなす．このように会員がどれだけの可能性で各潜在クラスjに所属するのかを負担率$r_{ij}$とよび，負担率をパラメータ推定の重みとしてかける．

負担率に基づく2つの正規分布のパラメータ推定と，2つの正規分布にもとづく負担率の計算を交互に行っていけば，次第に両者が良い値に近づくだろうというのがEMアルゴリズムの基本的な考え方となっている．

# 混合ガウスモデル
複数の正規分布が重なり合って作られる混合ガウスモデルを用いて潜在変数変数を含むモデルをどう定義するかをまとめる．

# Q関数

$$p(x,z|\theta)=\Pi_{i=1}^np(x^{(i)},z^{(i)}|\theta)$$

観測変数xと潜在変数zの値を共に知ることができるという仮想的な状況におけるデータを完全データとよび，観測変数xの値のみしかわからない状況におけるデータを不完全データとよぶ．
$p(x,z|\theta)$は完全データについて同時分布である．
zは未知の値であるが，パラメータの推定量$\hat{\theta}$と観測値により決まる$p(z|x,\hat{\theta})$を使って完全データ$\log p(x,z|\theta)$の期待値を求めたものをQ関数と呼ぶ．

$$Q(\theta,\hat{\theta})=\mathbb{E}_{p(z|x,\hat{\theta})}[\log p(x,z|\theta)]=\int p(z|x,\hat{\theta})\log p(x,z|\theta)dz$$

Q関数は完全データの対数尤度の期待値であり，EMアルゴリズムではQ関数の最大化を考える

# EMアルゴリズムによる混合ガウスモデルのパラメータ推定の更新式
1. $t=1$と設定，各jについて$\hat{\pi}_j^{(0)},\hat{\mu_j}^{(0)},\hat{\sigma^{2(0)}_j}$をランダムな値で初期化する．($\pi$はマルチヌーイ分布のパラメータベクトル，$\mu$は各正規分布の平均パラメータ，$\sigma^2$は各正規分布の分散パラメータ)

2. パラメータ値が収束するまで以下を繰り返す．

- Eステップ(期待値計算)

$$r_{ij}=\mathbb{E}_{p(z^{(i)}|x^{(i)},\hat{\theta})}[z_{ij}]$$

- Mステップ(最大化)$r_{ij}$を使いQ関数を最大化する$\hat{\pi}_j^{(t)},\hat{\mu}_j^{(t)}\hat{\sigma}_j^{2(t)}$を計算する．

$$Q(\theta,\hat{\theta})=\sum_{i=1}^n\sum_{j=1}^kr_{ij}(\log\pi_j+\log\mathcal{N}(x^{(i)}|\mu_j,\sigma_j^2))$$

より

$$R_j^{(t)}=\sum_{i=1}^{n}r_{ij}^{t}$$
$$\hat{\pi}_j^{(t)}=\frac{R_j^{(t)}}{\sum_{j=1}^kR_j^{(t)}}$$
$$\hat{\mu}_j^{(t)}=\frac{\sum_{i=1}^nr_{ij}^{(t)}x_i}{R_j^{(t)}}$$
$$\hat{\sigma}_j^{2(t)}=\frac{\sum_{i=1}^nr_{ij}^{(t)}(x_i-\hat{\mu}_j^{(t)})^2}{R_j^{(t)}}$$

- $t = t+1$

# 参考
- 手塚 太郎，"[しくみがわかるベイズ統計と機械学習](https://amzn.to/3cCILQM)"