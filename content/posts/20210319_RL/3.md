---
title: モデルフリー強化学習
date: 2021-03-19T12:00:23+09:00
draft: false
tags: ["Pythonで学ぶ強化学習"] 
---
<!--more-->
# モデルフリー強化学習
モデルフリーの強化学習では，モデル(遷移関数$T(s'|s,a)$・報酬関数$R(s,s')$)が未知であることが前提で，エージェント自らが動くことで経験を蓄積し，その経験から学習を行う．

## 探索と活用のトレードオフ
無限に行動することが可能であれば十分な探索の後に経験をかつようすればよい．しかし，多くの場合行動回数に何かしらの制限がある．
どれくらい調査の行動をして，どのくらい報酬目的の行動をするかを探索と活用のトレードオフとよぶ．トレードオフのバランスを取る手法として，Epsilon-Greedy法が一般的である．
- Epsilon-Greedy法  
Epsilonの確率で探索を行い，それ以外は活用目的の行動を行う．

## 計画の修正を実績で行うか，予測で行うか
計画の修正を実績に基づき行う場合と，予測により行う場合のトレードオフも存在する．  
前者の手法としてモンテカルロ法，後者の手法としてTD法，両者の間をとる手法としてTD($\lambda$)法がある．

### モンテカルロ法(時刻Tでエピソード修了)
エピソードが修了した後に，獲得できた報酬の総和をもとに修正を行うシンプルな方法である．

$$V(s_t) \leftarrow V(s_t)+\alpha((r_{t+1}+\gamma r_{t+2} + \gamma^2 r_{t+3} + ,...,\gamma^{T-1}r_T )-V(s_t))$$

1エピソードが終了する修正できない．

### TD法
学習する経験は，見積もり$V(s)$と実際$r+V(s')$の誤差である(TD誤差)．経験による修正は，以下のように価値の更新を行うこととなる．

$$V(s_t) \leftarrow V(s_t)+\alpha(r_{t+1}+\gamma V(s_{t+1})-V(s_t))$$

1stepごとに更新を行う手法がTD法となる．
修正期間を1より大きく，$T$より小さい値に設定する手法をMulti-step Learningとよぶ．

### TD($\lambda$)法
各stepにおける実際の価値を係数$\lambda$を使い合算した結果から，誤差を計算する手法である．
- 1step：$G_t^1=r_{t+1}+\gamma V(s_{t+1})$
- 2step：$G_t^2=r_{t+1}+\gamma r_{t+2}+\gamma^2 V(s_{t+2})$
- $T$step：$G_t^T=r_{t+1}+\gamma r_{t+2}+,...,+\gamma^{T-1}r_T$

$\lambda=1$でモンテカルロ法と等価なる．

## 価値ベースと方策ベース
モデルベース強化学習と同様に経験を価値評価の更新に用いるか方策の更新に用いるかで2つに分かれる．行動価値を$Q$で表す．
### 価値ベース(Off-policy)
- Q-learning  
  $$Q(s_t) \leftarrow Q(s_t)+\alpha(r_{t+1}+\gamma \max_a Q(s_{t+1})-Q(s_t))$$
### 方策ベース(On-policy)
- SARSA
  $$Q(s_t) \leftarrow Q(s_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1})-Q(s_t))$$

### Actor-Critic法
価値ベースと方策ベースを組み合わせた手法をActor-Critic法とよぶ．方策評価をActor，価値評価をCriticで相互に更新し学習を行う．

## 参考
久保隆宏,"[Pythonで学ぶ強化学習 入門から実践まで](https://amzn.to/3tA1S4W)"
