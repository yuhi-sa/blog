---
title: ニューラルネットを適応した強化学習
date: 2021-03-22T10:00:23+09:00
draft: false
tags: ["Pythonで学ぶ強化学習"] 
---
<!--more-->
# 学習安定化のための方法
## Experience Replay
行動履歴をプールしておき，そこからサンプリングすることで，さまざまなエピソードにおける，異なるタイムステップのデータを学習データとして使用する．これにより学習する経験の偏りを防ぎ，学習を安定化させる．

## Fixed Target Q-Network
一定期間固定されたパラメーターから価値を算出する手法．遷移先の価値を本体のモデルで計算する場合，学習のたびにパラメーターが変わるため，値が毎回変わることとなる．これではTD誤差が安定しないため，一定期間パラメーターを固定する．

## 報酬のClipping
全ゲームを通じ成功は1，失敗は-1と報酬を統一する．

# Deep Q-Network(DQN)とその改良
価値評価に深層学習を適応する方法をDeep Q-Network(DQN)とよぶ．単純にニューラルネットを使用するだけでは学習が安定しないため，前項のような安定化の工夫が必要となる．  
DQNを発表したDeep Mindは，改良手法6つを組み込んだRainbowというモデルを発表している．

## Double DQN
行動価値と行動選択のネットワークを分けることで<b>価値の見積もり精度</b>を上げる．

## Prioritized Replay
Experience Replyから単純にランダムサンプリングするのではなく，学習効果の高い(TD誤差が大きいもの)を優先してサンプリングを行い<b>学習効率</b>を上げる．

## Dueling Network
状態価値と行動価値を分けて計算することにより<b>価値の見積もり精度</b>を上げる．

## Multi-step Learning
Q学習とMonteCarlo法の間をとる手法で，「nステップ分の報酬」と「nステップ先の状態の価値」から修正を行うことで，<b>価値の見積もり精度</b>を上げる．

## Distributional RL
報酬を分布として扱い，<b>価値の見積もり精度</b>を上げる．

## Noisy Nets
Epsilon-Greedy法においてEpsilonの設定は非常にセンシティブである．Noisy Netsではどのくらいランダムに行動したほうがよいか自体をネットワークに学習することで<b>探索効率</b>を上げる．

# 参考
久保隆宏,"[Pythonで学ぶ強化学習 入門から実践まで](https://amzn.to/3tA1S4W)"