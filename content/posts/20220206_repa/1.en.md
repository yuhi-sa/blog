---
title: "Reparameterization trick"
date: 2022-02-06T15:00:23+09:00
draft: false
tags: ["Machine Learning"] 
---
<!--more-->
# Reparameterization trick
When variables are sampled from a probability distribution, they become random variables, and differentiation cannot be performed, making it impossible to perform calculations using backpropagation. To solve this problem, Kingma et al. introduced the technique called the reparameterization trick.

The reparameterization trick performs the same sampling by partitioning and combining the random numbers that are independent of the parameters and the parameters of the probability distribution from which they are sampled. For example, let $z$ be a variable sampled from a normal distribution $\mathcal{N}$ with mean $\mu$ and variance $\sigma$ as parameters:

$$z \sim \mathcal{N}(\mu, \sigma^2)$$

It can be transformed using random noise $\epsilon$ as follows:

$$z = \mu + \sigma \odot \epsilon$$

$$\epsilon \sim \mathcal{N}(0,1) $$

This makes $z$ a deterministic value with respect to $\mu$ and $\sigma$, making it differentiable.

![conceptual diagram](.././repara.png)

# Sampling using the Reparameterization Trick in PyTorch
```
m = Normal(*params)
z = m.rsample()
```
The probability distributions that can use sampling using the reparameterization trick are those that have the value True for:
```
m.has_rsample
```

# References
- [D. P. Kingma and M. Welling, “Auto-encoding variational bayes”, 2013.](https://arxiv.org/abs/1312.6114)
- [ReNom, 変分オートエンコーダ](https://www.renom.jp/ja/notebooks/tutorial/generative-model/VAE/notebook.html)
