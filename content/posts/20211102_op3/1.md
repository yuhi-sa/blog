---
title: "強化学習を用いてop3に歩行獲得させるROSパッケージのコード説明"
date: 2021-11-02T10:17:23+09:00
draft: false
tags: ["強化学習"] 
---
<!--more-->
# 強化学習を用いてop3に歩行獲得させるROSパッケージのコード説明
## はじめに
ROBOTIS OP3にGazeboシミュレーション内で強化学習を用いて歩行獲得させるROSパッケージのコード説明の記事です．  
- [op3_walk](https://github.com/yuhi-sa/op3_walk)

## 結果の動画
- [op3_controller_demo](https://github.com/yuhi-sa/op3_walk/blob/main/docs/op3_controller_demo.mp4)

## 手法の説明
深層強化学習(DQN)を使用しています．  
行動価値関数は，3層のニューラルネットワーク(NN)として定義しQ値を以下のように更新し，  

$Q(s_t,a_t) = Q(s_t,a_t) + \eta(R_{t+1)} + \gamma \max_a Q(s_{t+1},a) - Q(s_t,a_t)$

損失関数$L$を用いて誤差逆伝播しニューラルネットを更新しています．

$ L = \mathbb{E}(R_{t+1} + \gamma \max Q(s_{t+1},a_t)- Q(s_t,a_t))$

## プログラムの説明
### [function.py](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/function.py) and [motion.py](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/motion.py)

[function]にはエージェントの定義を書いています．  
Agentクラスが，ニューラルネットの定義をしているBrainクラスを持っています．ReplayMemoryクラスに蓄積される行動と状態で，Brainは損失の計算と更新を行います．行動は離散化しており，[motion]で定義されている行動の中から，epsilon-greedy選択を行います．  
こちらの書籍のコードを参考にしています．  
- [Deep-Reinforcement-Learning-Book](https://github.com/YutaroOgawa/Deep-Reinforcement-Learning-Book)

### [learning.py](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/learning.py)
[learning]では，[function]で定義したAgentクラスを継承します．Agentに，[controller]からsubscribeした状態を入力，行動を計算し，publishします．こちらはニューラルネットワークの定義にpytorchを使っているため，python3系で実行する必要があります．    

### [controller.py](https://github.com/yuhi-sa/op3_walk/blob/main/scripts/controller.py)
[controller]は．[learning]からpublishされた行動をsubscribeして，実際にop3を動かします．そして，状態をpublishします．こちらはop3のパッケージの関係で，python2系で実行する必要があります．

## 学習曲線
歩行距離の学習曲線
![歩行距離](https://github.com/yuhi-sa/op3_walk/blob/main/docs/learning.png?raw=true)
