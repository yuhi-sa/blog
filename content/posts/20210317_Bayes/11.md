---
title: 変分ベイズ
date: 2021-03-18T10:00:23+09:00
draft: false
tags: ["しくみがわかるベイズ統計と機械学習"] 
---
<!--more-->
# 変分ベイズ
変分ベイズは，EMアルゴリズムと同様に変分下界を使いつつパラメータ分布も考えることでベイズ的な拡張も行う手法である．
EMアルゴリズムではパラメータ$\theta$を引数とする変分下界$\mathcal{B}(\theta,\hat{\theta})$を考えたが変分ベイズでは変分下界の引数として近似分布qを考える．$\mathcal{B}(q)$を最大化する近似分布$q$を求めることが変分ベイズの目的となる．
- 変分：関数による汎関数の微分
- 汎関数：関数を引数とする関数

## 変分ベイズにおける変分下界
### EMアルゴリズムにおける変分下界

$$\mathcal{B}(\theta,\hat{\theta})=\log p(x|\theta)-\mathcal{D}(p(z|\hat{\theta})||p(z|x,\theta))$$

### 変分ベイズにおける変分下界

$$\mathcal{B}(q)=\log p(x) - \mathcal{D}(q(w)||p(w|x))$$

変分ベイズではパラメータと潜在変数をまとめて$w$で表すため，変分下界の引数として$\theta,\hat{\theta}$を使うことができない．そこで近似分布$q$自体を引数とする．
$x$は観測済みであるため，$p(x)$は定数である．変分下界を大きくするためには，近似分布$q(w)$を真の分布$p(w|x)$に近づけることでKLダイバージェンス$\mathcal{D}(q(w)||p(w|x))$を小さくする必要がある．

#### 変分下界の式変形
変分ベイズにおける変分下界を式変形すると以下のようになる．

$$\mathcal{B}(q)=\int q(w)\log \frac{p(x,w)}{q(w)}dw$$

よって期待値の形で以下のように表すことができる．

$$\mathcal{B}(q)=E_{q(w)}[\log p(x,w)] - E_{q(w)}[\log q(w)]$$

以上より，2つの対数尤度の差の最小化問題として扱うことができる．

## 参考
- 手塚 太郎，"[しくみがわかるベイズ統計と機械学習](https://amzn.to/3cCILQM)"
