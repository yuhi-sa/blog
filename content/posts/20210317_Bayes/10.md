---
title: EMアルゴリズム2(変分下界とKLダイバージェンス)
date: 2021-03-17T13:05:23+09:00
draft: false
tags: ["しくみがわかるベイズ統計と機械学習"] 
---
<!--more-->
# EMアルゴリズム2(変分下界とKLダイバージェンス)
EステップとMステップを繰り返すだけで，EMアルゴリズムがうまく推定できる理由について理解するために，変分下界とKLダイバージェンスという概念を導入する．

## 変分下界
EMアルゴリズムにおける変分下界は以下のように定義される．

$$\mathcal{B}(\theta,\hat{\theta})=\int p(z|x,\hat{\theta}) \log \frac{p(x,z|\theta)}{p(z|x,\hat{\theta})}dz$$

変分下界の式を変形する

$$\mathcal{B}(\theta,\hat{\theta})=\log p(x|\theta)-(\int p(z|x,\hat{\theta}\log\frac{p(z|x,\hat{\theta})}{p(z|x,\theta)}dz)$$

これは第一項が，対数尤度，第二項がKLダイバージェンスを表している．
これにより変分下界はQ関数とエントロピーの和となっていることがわかる．

$$\mathcal{B}(\theta,\hat{\theta})= Q(\theta,\hat{\theta})+H(p(z|x,\hat{\theta}))$$

### KLダイバージェンス

KLダイバージェンス$\mathcal{D}(q||p)$は，分布qと分布pの比の対数の期待値として定義され，分布どうしの距離を測るために使われる．

$$\mathcal{D}(q||p)=\mathbb{E}[\log\frac{q}{p}]\int q(x)\log \frac{q(x)}{p(x)}dx$$

2つの分布が類似しているほどKLダイバージェンスは小さくなり，完全に一致する場合0となる．また，Jensenの不等式より非負性が証明されている．

![変分下界](./../変分下界.png)

- $\hat{\theta}$の更新
分布が一致し，KLダイバージェンスが0になるため変分下界$\mathcal{B}$が増加する．
- $\theta$の更新
Q関数を最大化することにより，変分下界$\mathcal{B}$も増加する．エントロピー$H$は$\theta$が含まれないため変化しない．

以上より，EMアルゴリズムによって変分下界が増加していく．変分下界とKLダイバージェンスの和である対数尤度は常に変分下界より大きいため，対数尤度も増加していく．よってEMアルゴリズムは対数尤度を増加させる．

## 参考
- 手塚 太郎，"[しくみがわかるベイズ統計と機械学習](https://amzn.to/3cCILQM)"
